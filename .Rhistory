idx_not_nan = idx_not_nan_center)
idx_not_nan_center = matrix(data = TRUE,nrow=length(lon_center),ncol=length(lat_center))
for (t in 1:length(Xt_center)) {
tmp = !is.na(Xt_center[[t]])
idx_not_nan_center = idx_not_nan_center & tmp
}
predictor = cv_EK_2d( X = train_set,
grid_eval1 = lon_center,
grid_eval2 = lat_center,
p_vector = p_vec,
improved = improved_ke,
idx_not_nan = idx_not_nan_center)
idx_not_nan_center = matrix(data = TRUE,nrow=length(lon_center),ncol=length(lat_center))
for (t in 1:length(Xt_center)) {
tmp = !is.na(Xt_center[[t]])
idx_not_nan_center = idx_not_nan_center & tmp
}
wjkxnlke
denwbqdkjnlwk2jdlk2l√≤kd2
View(idx_not_nan_center)
rm(list=ls())
graphics.off()
cat("\014")
set.seed(23032000)
library(PPCKO)
#change here
dir_w = "/Users/andreafranzoni/Documents/Politecnico/Magistrale/Tesi/Functional_time_series_forecasting"
#save res
save_res = TRUE
folder_res = "/KE"
#where the data are
dir_data = paste0(dir_w,"/Test_domain2D/RealWorld_data/data")
path_data_mouth = paste0(dir_data,"/mouth")
path_data_center = paste0(dir_data,"/center")
#where to store results
dir_res = paste0(dir_w,"/Test_domain2D/RealWorld_data/results/results_prediction")
path_store_res = paste0(dir_res,folder_res)
#load functions to use KE
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/EstimatedKernel_predictor_2D.R"))       #load parameter to generate data according to a strategy
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/KE_cv_2D.R"))       #load parameter to generate data according to a strategy
#load the dataset
load(paste0(dir_data,"/BS.Rdata"))
#load data for mouth zone
load(paste0(path_data_mouth,"/BS_mouth.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_1.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_2.Rdata"))
#load data for center zone
load(paste0(path_data_center,"/BS_center.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_1.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_2.Rdata"))
first_day_first_train = "20170101"
idx_first_day_first_train = which(dates==first_day_first_train)
window_train_set = 99
days_to_be_pred = 1000
#store results
prediction_KE_mouth          <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_1   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_2   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center         <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_1  <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_2  <- lapply(1:days_to_be_pred,function(x) NULL)
# paramters for KE
improved_ke = FALSE
p_vec = c(2,3,4,5,6)
idx_not_nan_mouth = matrix(data = TRUE,nrow=length(lon_mouth),ncol=length(lat_mouth))
for (t in 1:length(Xt_mouth)) {
tmp = !is.na(Xt_mouth[[t]])
idx_not_nan_mouth = idx_not_nan_mouth & tmp
}
idx_not_nan_center = matrix(data = TRUE,nrow=length(lon_center),ncol=length(lat_center))
for (t in 1:length(Xt_center)) {
tmp = !is.na(Xt_center[[t]])
idx_not_nan_center = idx_not_nan_center & tmp
}
View(idx_not_nan_mouth)
View(idx_not_nan_center)
i=1
train_set = Xt_center[(i - 1 + idx_first_day_first_train):(i - 1 + idx_first_day_first_train + window_train_set)]
idx_not_nan = indexes_not_NA[lon_center]
X = train_set
grid_eval1 = lon_center
grid_eval2 = lat_center
p_vector = p_vec
improved = improved_ke
idx_not_nan = idx_not_nan_center
View(idx_not_nan)
#X=train_set,grid_eval=t.grid,p_vector=c(2,3,4,5,6),improved = TRUE
n = length(X)
t_start = ceiling(n/2)
t_end = n-1
time_instants_cv = seq(t_start,t_end,by=1)
n_cv_iter = length(time_instants_cv)
err_cv = numeric(length(p_vector))
j=1
p = p_vector[j]
mse_val = numeric(n_cv_iter)
count_err = 1
t=50
X_train = X[1:t]
X_val   = as.vector(X[t+1][[1]])
# From list of matrices to list of lists of vectors --------------------------
Xt_list_of_vec = vector("list", length = t)
# for each time
for(i in 1:(t))
{
Xt_list_of_vec[[i]] = vector("list", length = 1)
Xt_list_of_vec[[i]][[1]] = as.vector(X_train[[i]])
}
data_y=Xt_list_of_vec,
data_y=Xt_list_of_vec
indexes_not_NA=idx_not_nan
l=3
b=2
seed_split=FALSE
x1.grid=grid_eval1
x2.grid=grid_eval2
FPCA_method="discretization"
basis.type="bspline"
grid.step.num.int=0.001
nbasis.1=NULL
nbasis.2=NULL
cum_prop_var=NULL
nharm=p
center=TRUE
EK_improved=improved
if (is.null(seed_split)==TRUE || (seed_split!=FALSE & is.numeric(seed_split)==FALSE)) stop("Argument 'seed_split' must be either FALSE or an integer.")
if(is.list(data_y)==FALSE || is.data.frame(data_y)==TRUE || is.list(data_y[[1]])==FALSE || is.data.frame(data_y[[1]])==TRUE){ #le prime due condizioni son per assicurarsi che sia una lista nel senso di "list" e non un dataframe (che ? una lista tecnicamente),
stop("data_y must be a list of lists. Specifically, data_y must be a list of 'n' lists. Each of the 'n' lists must be made up of 'p' lists. Each of the 'p' lists must contain a numeric vector expressing the evaluation of the function on a grid (whose length can be different in the 'p' dimensions).
'n' (i.e. the sample size) must be greater or equal than 2. 'p'(i.e. the dimension of the multivariate function ) must be the same for all the multivariate functions.")} else{
n=length(data_y)
p=length(data_y[[1]])
grid_size=vapply(data_y[[1]],function(x) length(x),integer(1))
}
if (b<=0 || (b %% 1) !=0) stop("'b' must be a positive integer.")
if (n <2) stop("'n'(i.e. the sample size) must be greater or equal than 2.")
if (length(unique(vapply(data_y,length,integer(1))))!=1) stop("'p'(i.e. the dimension of the multivariate function) must be the same for all the multivariate functions.")
if(!(all(apply(t(vapply(data_y,function(x) vapply(x,length,integer(1)),integer(p))),2, function(y) length(unique(y))==1)))) stop("The 'n' functions must be evaluated on the same p-variate grid. The grid can vary between the p domains.")
if ((l+1) %% b !=0 || (l+1)/b-1 <=0 ) stop("(l+1)/b must be an integer >=2.")
# DIVISION TRAINING-CALIBRATION ----------------------------------------------
fitted_order=2 # TODO: change!!!
burn_in=fitted_order
m=n-l-burn_in
if(seed_split!=FALSE){set.seed(seed_split)}
training=sample((burn_in+1):n,m)
training=sort(training)
calibration=setdiff((burn_in+1):n,training)
calibration=sort(calibration)
excluded_set=1:burn_in
# total number of points in the grid
if(is.null(indexes_not_NA)){
n_points = length(x1.grid) * length(x2.grid)
} else {
n_points = sum(indexes_not_NA)
}
n_points
# Vectorize data
Xt_mat = vapply(simplify2array(data_y, as.numeric), as.numeric, numeric(length=n_points)) # n_points x n
simplify2array(data_y, as.numeric)
simplify2array(data_y, as.numeric)
# Vectorize data
Xt_mat = vapply(simplify2array(data_y, as.numeric), as.numeric, numeric(length=n_points)) # n_points x n
rm(list=ls())
graphics.off()
cat("\014")
set.seed(23032000)
library(PPCKO)
#change here
dir_w = "/Users/andreafranzoni/Documents/Politecnico/Magistrale/Tesi/Functional_time_series_forecasting"
#save res
save_res = TRUE
folder_res = "/KE"
#where the data are
dir_data = paste0(dir_w,"/Test_domain2D/RealWorld_data/data")
path_data_mouth = paste0(dir_data,"/mouth")
path_data_center = paste0(dir_data,"/center")
#where to store results
dir_res = paste0(dir_w,"/Test_domain2D/RealWorld_data/results/results_prediction")
path_store_res = paste0(dir_res,folder_res)
#load functions to use KE
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/EstimatedKernel_predictor_2D.R"))       #load parameter to generate data according to a strategy
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/KE_cv_2D.R"))       #load parameter to generate data according to a strategy
#load the dataset
load(paste0(dir_data,"/BS.Rdata"))
#load data for mouth zone
load(paste0(path_data_mouth,"/BS_mouth.Rdata"))
View(indexes_not_NA)
load(paste0(path_data_mouth,"/BS_mouth_diff_1.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_2.Rdata"))
#load data for center zone
load(paste0(path_data_center,"/BS_center.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_1.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_2.Rdata"))
first_day_first_train = "20170101"
idx_first_day_first_train = which(dates==first_day_first_train)
window_train_set = 99
days_to_be_pred = 1000
#store results
prediction_KE_mouth          <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_1   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_2   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center         <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_1  <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_2  <- lapply(1:days_to_be_pred,function(x) NULL)
# paramters for KE
improved_ke = FALSE
p_vec = c(2,3,4,5,6)
idx_not_nan_mouth = matrix(data = TRUE,nrow=length(lon_mouth),ncol=length(lat_mouth))
for (t in 1:length(Xt_mouth)) {
tmp = !is.na(Xt_mouth[[t]])
idx_not_nan_mouth = idx_not_nan_mouth & tmp
}
idx_not_nan_center = matrix(data = TRUE,nrow=length(lon_center),ncol=length(lat_center))
for (t in 1:length(Xt_center)) {
tmp = !is.na(Xt_center[[t]])
idx_not_nan_center = idx_not_nan_center & tmp
}
View(idx_not_nan_mouth)
Xt_mouth[[1]]
rm(list=ls())
graphics.off()
cat("\014")
set.seed(23032000)
library(PPCKO)
#change here
dir_w = "/Users/andreafranzoni/Documents/Politecnico/Magistrale/Tesi/Functional_time_series_forecasting"
#save res
save_res = TRUE
folder_res = "/KE"
#where the data are
dir_data = paste0(dir_w,"/Test_domain2D/RealWorld_data/data")
path_data_mouth = paste0(dir_data,"/mouth")
path_data_center = paste0(dir_data,"/center")
#where to store results
dir_res = paste0(dir_w,"/Test_domain2D/RealWorld_data/results/results_prediction")
path_store_res = paste0(dir_res,folder_res)
#load functions to use KE
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/EstimatedKernel_predictor_2D.R"))       #load parameter to generate data according to a strategy
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/KE_cv_2D.R"))       #load parameter to generate data according to a strategy
#load the dataset
load(paste0(dir_data,"/BS.Rdata"))
#load data for mouth zone
load(paste0(path_data_mouth,"/BS_mouth.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_1.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_2.Rdata"))
#load data for center zone
load(paste0(path_data_center,"/BS_center.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_1.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_2.Rdata"))
first_day_first_train = "20170101"
idx_first_day_first_train = which(dates==first_day_first_train)
window_train_set = 99
days_to_be_pred = 1000
#store results
prediction_KE_mouth          <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_1   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_2   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center         <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_1  <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_2  <- lapply(1:days_to_be_pred,function(x) NULL)
# paramters for KE
improved_ke = FALSE
p_vec = c(2,3,4,5,6)
idx_not_nan_mouth = matrix(data = TRUE,nrow=length(lon_mouth),ncol=length(lat_mouth))
for (t in 1:length(Xt_mouth)) {
tmp = !is.na(Xt_mouth[[t]])
idx_not_nan_mouth = idx_not_nan_mouth & tmp
}
idx_not_nan_center = matrix(data = TRUE,nrow=length(lon_center),ncol=length(lat_center))
for (t in 1:length(Xt_center)) {
tmp = !is.na(Xt_center[[t]])
idx_not_nan_center = idx_not_nan_center & tmp
}
i=1
train_set = Xt_center[(i - 1 + idx_first_day_first_train):(i - 1 + idx_first_day_first_train + window_train_set)]
idx_not_nan = indexes_not_NA[lon_center]
X = train_set
grid_eval1 = lon_center
grid_eval2 = lat_center
p_vector = p_vec
improved = improved_ke
idx_not_nan = idx_not_nan_center
View(idx_not_nan_center)
#X=train_set,grid_eval=t.grid,p_vector=c(2,3,4,5,6),improved = TRUE
n = length(X)
t_start = ceiling(n/2)
t_end = n-1
time_instants_cv = seq(t_start,t_end,by=1)
n_cv_iter = length(time_instants_cv)
err_cv = numeric(length(p_vector))
j=1
p = p_vector[j]
mse_val = numeric(n_cv_iter)
count_err = 1
t=50
X_train = X[1:t]
X_val   = as.vector(X[t+1][[1]])
# From list of matrices to list of lists of vectors --------------------------
Xt_list_of_vec = vector("list", length = t)
# for each time
for(i in 1:(t))
{
Xt_list_of_vec[[i]] = vector("list", length = 1)
Xt_list_of_vec[[i]][[1]] = as.vector(X_train[[i]])
}
View(Xt_list_of_vec)
data_y=Xt_list_of_vec
data_y=Xt_list_of_vec
indexes_not_NA=idx_not_nan
l=3
b=2
seed_split=FALSE,
seed_split=FALSE
x1.grid=grid_eval1
x2.grid=grid_eval2
FPCA_method="discretization"
basis.type="bspline"
grid.step.num.int=0.001
nbasis.1=NULL
nbasis.2=NULL
cum_prop_var=NULL
nharm=p
center=TRUE
EK_improved=improved
if (is.null(seed_split)==TRUE || (seed_split!=FALSE & is.numeric(seed_split)==FALSE)) stop("Argument 'seed_split' must be either FALSE or an integer.")
if(is.list(data_y)==FALSE || is.data.frame(data_y)==TRUE || is.list(data_y[[1]])==FALSE || is.data.frame(data_y[[1]])==TRUE){ #le prime due condizioni son per assicurarsi che sia una lista nel senso di "list" e non un dataframe (che ? una lista tecnicamente),
stop("data_y must be a list of lists. Specifically, data_y must be a list of 'n' lists. Each of the 'n' lists must be made up of 'p' lists. Each of the 'p' lists must contain a numeric vector expressing the evaluation of the function on a grid (whose length can be different in the 'p' dimensions).
'n' (i.e. the sample size) must be greater or equal than 2. 'p'(i.e. the dimension of the multivariate function ) must be the same for all the multivariate functions.")} else{
n=length(data_y)
p=length(data_y[[1]])
grid_size=vapply(data_y[[1]],function(x) length(x),integer(1))
}
if (b<=0 || (b %% 1) !=0) stop("'b' must be a positive integer.")
if (n <2) stop("'n'(i.e. the sample size) must be greater or equal than 2.")
if (length(unique(vapply(data_y,length,integer(1))))!=1) stop("'p'(i.e. the dimension of the multivariate function) must be the same for all the multivariate functions.")
if(!(all(apply(t(vapply(data_y,function(x) vapply(x,length,integer(1)),integer(p))),2, function(y) length(unique(y))==1)))) stop("The 'n' functions must be evaluated on the same p-variate grid. The grid can vary between the p domains.")
if ((l+1) %% b !=0 || (l+1)/b-1 <=0 ) stop("(l+1)/b must be an integer >=2.")
# DIVISION TRAINING-CALIBRATION ----------------------------------------------
fitted_order=2 # TODO: change!!!
burn_in=fitted_order
m=n-l-burn_in
if(seed_split!=FALSE){set.seed(seed_split)}
training=sample((burn_in+1):n,m)
training=sort(training)
calibration=setdiff((burn_in+1):n,training)
calibration=sort(calibration)
excluded_set=1:burn_in
# total number of points in the grid
if(is.null(indexes_not_NA)){
n_points = length(x1.grid) * length(x2.grid)
} else {
n_points = sum(indexes_not_NA)
}
# Vectorize data
Xt_mat = vapply(simplify2array(data_y, as.numeric), as.numeric, numeric(length=n_points)) # n_points x n
simplify2array(data_y, as.numeric)
# Vectorize data
Xt_mat = vapply(simplify2array(data_y, as.numeric), as.numeric, numeric(length=n_points)) # n_points x n
data_y[indexes_not_NA]
data_y
simplify2array(data_y, as.numeric)
rm(list=ls())
graphics.off()
cat("\014")
set.seed(23032000)
library(PPCKO)
#change here
dir_w = "/Users/andreafranzoni/Documents/Politecnico/Magistrale/Tesi/Functional_time_series_forecasting"
#save res
save_res = TRUE
folder_res = "/KE"
#where the data are
dir_data = paste0(dir_w,"/Test_domain2D/RealWorld_data/data")
path_data_mouth = paste0(dir_data,"/mouth")
path_data_center = paste0(dir_data,"/center")
#where to store results
dir_res = paste0(dir_w,"/Test_domain2D/RealWorld_data/results/results_prediction")
path_store_res = paste0(dir_res,folder_res)
#load functions to use KE
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/EstimatedKernel_predictor_2D.R"))       #load parameter to generate data according to a strategy
source(paste0(dir_w,"/Test_domain2D/RealWorld_data/utils/KE_cv_2D.R"))       #load parameter to generate data according to a strategy
#load the dataset
load(paste0(dir_data,"/BS.Rdata"))
#load data for mouth zone
load(paste0(path_data_mouth,"/BS_mouth.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_1.Rdata"))
load(paste0(path_data_mouth,"/BS_mouth_diff_2.Rdata"))
#load data for center zone
load(paste0(path_data_center,"/BS_center.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_1.Rdata"))
load(paste0(path_data_center,"/BS_center_diff_2.Rdata"))
first_day_first_train = "20170101"
idx_first_day_first_train = which(dates==first_day_first_train)
window_train_set = 99
days_to_be_pred = 1000
#store results
prediction_KE_mouth          <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_1   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_mouth_diff_2   <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center         <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_1  <- lapply(1:days_to_be_pred,function(x) NULL)
prediction_KE_center_diff_2  <- lapply(1:days_to_be_pred,function(x) NULL)
# paramters for KE
improved_ke = FALSE
p_vec = c(2,3,4,5,6)
i=1
train_set = Xt_center[(i - 1 + idx_first_day_first_train):(i - 1 + idx_first_day_first_train + window_train_set)]
X = train_set
grid_eval1 = lon_center
grid_eval2 = lat_center
p_vector = p_vec
improved = improved_ke
#X=train_set,grid_eval=t.grid,p_vector=c(2,3,4,5,6),improved = TRUE
n = length(X)
idx_not_nan = matrix(data = TRUE,nrow=length(grid_eval1),ncol=length(grid_eval2))
for (t in 1:length(X)) {
idx_not_nan = idx_not_nan & !is.na(X[[t]])
}
View(idx_not_nan)
t_start = ceiling(n/2)
t_end = n-1
time_instants_cv = seq(t_start,t_end,by=1)
n_cv_iter = length(time_instants_cv)
err_cv = numeric(length(p_vector))
j=1
p = p_vector[j]
mse_val = numeric(n_cv_iter)
count_err = 1
t=50
X_train = X[1:t]
X_val   = as.vector(X[t+1][[1]])
# From list of matrices to list of lists of vectors --------------------------
Xt_list_of_vec = vector("list", length = t)
# for each time
for(i in 1:(t))
{
Xt_list_of_vec[[i]] = vector("list", length = 1)
Xt_list_of_vec[[i]][[1]] = as.vector(X_train[[i]])
}
testing = EK_pred_2D(data_y=Xt_list_of_vec,
indexes_not_NA=idx_not_nan,
l=3,
b=2,
seed_split=FALSE,
x1.grid=grid_eval1,
x2.grid=grid_eval2,
FPCA_method="discretization",
basis.type="bspline",
grid.step.num.int=0.001,
nbasis.1=NULL,
nbasis.2=NULL,
cum_prop_var=NULL,
nharm=p,
center=TRUE,
EK_improved=improved)
data_y=Xt_list_of_vec
indexes_not_NA=idx_not_nan
l=3
b=2
seed_split=FALSE
x1.grid=grid_eval1
x2.grid=grid_eval2
FPCA_method="discretization"
basis.type="bspline"
grid.step.num.int=0.001
nbasis.1=NULL
nbasis.2=NULL
cum_prop_var=NULL
nharm=p
center=TRUE
EK_improved=improved
if (is.null(seed_split)==TRUE || (seed_split!=FALSE & is.numeric(seed_split)==FALSE)) stop("Argument 'seed_split' must be either FALSE or an integer.")
if(is.list(data_y)==FALSE || is.data.frame(data_y)==TRUE || is.list(data_y[[1]])==FALSE || is.data.frame(data_y[[1]])==TRUE){ #le prime due condizioni son per assicurarsi che sia una lista nel senso di "list" e non un dataframe (che ? una lista tecnicamente),
stop("data_y must be a list of lists. Specifically, data_y must be a list of 'n' lists. Each of the 'n' lists must be made up of 'p' lists. Each of the 'p' lists must contain a numeric vector expressing the evaluation of the function on a grid (whose length can be different in the 'p' dimensions).
'n' (i.e. the sample size) must be greater or equal than 2. 'p'(i.e. the dimension of the multivariate function ) must be the same for all the multivariate functions.")} else{
n=length(data_y)
p=length(data_y[[1]])
grid_size=vapply(data_y[[1]],function(x) length(x),integer(1))
}
if (b<=0 || (b %% 1) !=0) stop("'b' must be a positive integer.")
if (n <2) stop("'n'(i.e. the sample size) must be greater or equal than 2.")
if (length(unique(vapply(data_y,length,integer(1))))!=1) stop("'p'(i.e. the dimension of the multivariate function) must be the same for all the multivariate functions.")
if(!(all(apply(t(vapply(data_y,function(x) vapply(x,length,integer(1)),integer(p))),2, function(y) length(unique(y))==1)))) stop("The 'n' functions must be evaluated on the same p-variate grid. The grid can vary between the p domains.")
if ((l+1) %% b !=0 || (l+1)/b-1 <=0 ) stop("(l+1)/b must be an integer >=2.")
# DIVISION TRAINING-CALIBRATION ----------------------------------------------
fitted_order=2 # TODO: change!!!
burn_in=fitted_order
m=n-l-burn_in
if(seed_split!=FALSE){set.seed(seed_split)}
training=sample((burn_in+1):n,m)
training=sort(training)
calibration=setdiff((burn_in+1):n,training)
calibration=sort(calibration)
excluded_set=1:burn_in
# total number of points in the grid
if(is.null(indexes_not_NA)){
n_points = length(x1.grid) * length(x2.grid)
} else {
n_points = sum(indexes_not_NA)
}
# Vectorize data
Xt_mat = vapply(simplify2array(data_y, as.numeric), as.numeric, numeric(length=n_points)) # n_points x n
library(PPCKO)
#DO NOT CHANGE FROM HERE
#to uploda the package
#change here the directory
setwd("/Users/andreafranzoni/Documents/Politecnico/Magistrale/PACS/pacs-project/PPCforAutoregressiveOperator")
#then
Rcpp::compileAttributes(".")
library(devtools)
remove.packages("PPCKO")
remove.packages("PPCKO.local")
remove.packages("PPCKO.local2")
